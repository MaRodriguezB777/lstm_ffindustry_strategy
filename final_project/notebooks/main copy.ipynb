{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import statsmodels.api as sm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","from xgboost import XGBRegressor, XGBRFRegressor\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def load_ff3(val_start_date=None):\n","    ff3 = pd.read_csv('ff3_daily.csv')\n","    \n","    if val_start_date is not None:\n","        ff3 = ff3[ff3['date'] >= val_start_date]\n","        \n","    ff3 = ff3.reset_index(drop=True)\n","    ff3.index = ff3['date']\n","    ff3 = ff3.drop(columns=['date'])\n","    \n","    return ff3\n","\n","def set_seed(seed_value):\n","    # Adding a fixed seed from this solution: https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras\n","    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n","    os.environ['PYTHONHASHSEED']=str(seed_value)\n","\n","    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n","    random.seed(seed_value)\n","\n","    # 3. Set the `numpy` pseudo-random generator at a fixed value\n","    np.random.seed(seed_value)\n","\n","    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n","    # tf.compat.v1.set_random_seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    return\n","\n","def io_day_n_lag(data, n):\n","    # for each column in data, create five lags\n","    y = data.iloc[n:,:].reset_index(drop=True)    \n","    \n","    X = pd.DataFrame()\n","\n","    for i in range(1, n+1):\n","        temp = data.shift(i).reset_index(drop=True)\n","        temp.columns = [f'{col}_lag{i}' for col in temp.columns]\n","        X = pd.concat([X, temp], axis=1)\n","    \n","    X = X.iloc[n:,:].reset_index(drop=True)\n","    \n","    return X, y\n","\n","# def io_day_1_lag_second_order_input(data):\n","#     # Create input output pairs where input data includes second order interactions\n","#     X, y = io_day_1_lag(data)\n","#     cols = X.columns\n","\n","#     for i in range(len(cols)):\n","#         for j in range(i+1, len(cols)):\n","#             col_name = cols[i] + cols[j]\n","#             col_values = X[cols[i]] * X[cols[j]]\n","#             X[col_name] = col_values\n","#     return X, y\n","\n","def predictions_to_returns(pred_df, y, weighted=True):\n","    # Given the predictions of each factor for each day, calculate our\n","    # strategy for each day, and the returns for each day\n","\n","    # Apply our strategy to our predictions (in form [0, 1, 0, 0, 0])\n","    if weighted:\n","        strat_df = pred_df.apply(lambda row : weighted_predicted_factor_strat(row), axis = 1)\n","    else:\n","        strat_df = pred_df.apply(lambda row : max_predicted_factor_strat(row), axis = 1)\n","\n","    # Calculate our returns\n","    return_vector = np.multiply(strat_df,np.asarray(y)).apply(sum, axis = 1)\n","\n","    return strat_df, return_vector\n","\n","def weighted_predicted_factor_strat(row):\n","    pos_preds = [x for x in row if x > 0]\n","    neg_preds = [x for x in row if x < 0]\n","    pos_sum = sum(pos_preds)\n","    neg_sum = abs(sum(neg_preds))\n","    row_list = [0]*len(row)\n","    for i in range(len(row)):\n","        if row[i] > 0:\n","            row_list[i] = row[i] / pos_sum\n","        elif row[i] < 0:\n","            row_list[i] = row[i] / neg_sum\n","    return pd.Series(row_list)\n","    \n","\n","def max_predicted_factor_strat(row):\n","    # For each day, set our strategy to be the factor with\n","    # the highest predicted return\n","    max_pred_return = max(row)\n","    row_list = [x == max_pred_return for x in row]\n","    return pd.Series(row_list)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class MultiLayerLSTM(nn.Module):\n","    def __init__(self, input_size, output_size, params=None):\n","        super(MultiLayerLSTM, self).__init__()\n","        \n","        self.hidden_sizes = params[\"hidden_sizes\"] if params and \"hidden_sizes\" in params else [50]\n","        self.dropout = params[\"dropout\"] if params and \"dropout\" in params else 0\n","        self.activation_fn = params[\"activation_fn\"] if params and \"activation_fn\" in params else nn.ReLU()\n","        \n","        self.lstm_layers = nn.ModuleList()\n","        self.lstm_layers.append(nn.LSTM(input_size, self.hidden_sizes[0], dropout=self.dropout, bidirectional=True, batch_first=True))\n","        \n","        for i in range(1, len(self.hidden_sizes)):\n","            self.lstm_layers.append(nn.LSTM(self.hidden_sizes[i-1] * 2, self.hidden_sizes[i], dropout=self.dropout, bidirectional=True, batch_first=True))\n","            \n","        self.output_layer = nn.Linear(self.hidden_sizes[-1] * 2, output_size)\n","    \n","    def forward(self, x):\n","        h_t = x\n","        for lstm_layer in self.lstm_layers:\n","            h_t, _ = lstm_layer(h_t)\n","            h_t = h_t[:, -1, :]\n","            h_t = self.activation_fn(h_t)\n","        \n","        # Only use the output of the last LSTM layer\n","        out = self.output_layer(h_t)\n","        return out"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class SimpleLSTM(nn.Module):\n","    def __init__(self, input_size, output_size, params=None):\n","        super(SimpleLSTM, self).__init__()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        \n","        self.hidden_size = params[\"hidden_size\"] if params and \"hidden_size\" in params else 50\n","        self.num_layers = params[\"num_layers\"] if params and \"num_layers\" in params else 1\n","        self.dropout = params[\"dropout\"] if params and \"dropout\" in params else 0\n","        self.activation_fn = params[\"activation_fn\"] if params and \"activation_fn\" in params else nn.ReLU()\n","        \n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True)\n","        self.fc_linear =  nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, x):\n","        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) # hidden state\n","        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) # internal state\n","        \n","        # Propagate input through LSTM\n","        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # lstm with input, hidden, and internal state\n","        hn = hn.view(-1, self.hidden_size) # reshaping the data for Dense layer next\n","        out = self.activation_fn(hn)\n","        out = self.fc_linear(out)\n","        return out"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class SimpleFeedForwardNN(nn.Module):\n","    def __init__(self, input_size, hidden_sizes, output_size):\n","        super(SimpleFeedForwardNN, self).__init__()\n","        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_sizes[0])])\n","        self.hidden_layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]) for i in range(len(hidden_sizes)-1)])\n","        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n","\n","        print(f\"number of hidden layers: {len(hidden_sizes)}\")\n","    def forward(self, x):\n","        for layer in self.hidden_layers:\n","            x = nn.functional.relu(layer(x))\n","        x = self.output_layer(x)\n","        return x"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def save_checkpoint(model, optimizer, epoch, train_losses, val_losses, lr, batch_size, weighting_type, path):\n","    checkpoint = {\n","        \"epoch\": epoch,\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","        \"train_losses\": train_losses,\n","        \"val_losses\": val_losses,\n","        \"lr\": lr,\n","        \"batch_size\": batch_size,\n","        \"weighting_type\": weighting_type\n","    }\n","    try:\n","        torch.save(checkpoint, path)\n","        print(f\"Checkpoint saved at {path}\")\n","    except Exception as e:\n","        print(f\"Error saving checkpoint: {e}\")\n","    \n","def load_checkpoint_from_path(path, input_size, output_size, model_type=SimpleLSTM, model_params=None):\n","    print(f\"Loading checkpoint from {path}.pt\")\\\n","            \n","    checkpoint = torch.load(path)\n","    model = model_type(input_size, output_size, params=model_params)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer = optim.Adam(model.parameters(), lr=checkpoint['lr'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    train_losses = checkpoint['train_losses']\n","    val_losses = checkpoint['val_losses']\n","    \n","    print(f\"Checkpoint loaded. \")\n","    \n","    return model, optimizer, train_losses, val_losses\n","\n","def load_model_LSTM(path):\n","    final_save = torch.load(path)\n","    \n","    model_state_dict = final_save[\"model_state_dict\"]\n","    model_params = final_save[\"model_params\"]\n","    model_type = final_save[\"model_type\"]\n","    input_size = final_save[\"input_size\"]\n","    output_size = final_save[\"output_size\"]\n","    \n","    model = model_type(input_size, output_size, params=model_params)\n","    model.load_state_dict(model_state_dict)\n","    \n","    return model\n","\n","def load_model_xgb(path):\n","    final_save = torch.load(path)\n","    \n","    model = final_save['model']\n","    \n","    return model"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def train_model_LSTM(\n","        X_train, \n","        y_train, \n","        X_val, \n","        y_val, \n","        num_epochs=50, \n","        batch_size=32, \n","        logging=True, \n","        save_epochs=None, \n","        final_save_path=None,\n","        checkpoints_dir=None, \n","        load_checkpoint=None,\n","        model_params=None,\n","        model_type=SimpleLSTM):\n","    if (save_epochs and not checkpoints_dir) or (checkpoints_dir and not save_epochs):\n","        raise ValueError(\"Both save_epochs and checkpoints_dir must be provided if one is provided\")\n","    elif checkpoints_dir:\n","        os.makedirs(checkpoints_dir, exist_ok=True)\n","    \n","    # Prapare datasets\n","    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n","    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n","    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n","    \n","    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","    \n","    # Calculate weights for each datapoint (closer to the end of the dataset, the more weight it has)\n","    def calc_weights():\n","        weights = torch.ones(len(y_train))\n","        for i in range(len(y_train)):\n","            weights[i] += 0.5 * (i / len(y_train))**2\n","        weights = weights / weights.sum()\n","        return weights\n","    # weights = calc_weights()\n","    weights = torch.ones(len(y_train))\n","    \n","    # model = Sequential()\n","    # model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n","    # model.add(Dense(16, activation='relu'))\n","    # model.add(Dense(6, activation='linear'))\n","    # model.compile(loss='mean_squared_error', optimizer='adam')\n","    # model.fit(X_train, y_train, batch_size=32, epochs=50,\n","    #           validation_data=(X_val, y_val), verbose=2)\n","    \n","    # Define LSTM model\n","    input_size = X_train.shape[-1]  # Number of features\n","    output_size = y_train.shape[-1]  # Number of output features\n","    start_epoch = 1\n","    if load_checkpoint and checkpoints_dir:\n","        model, optimizer, train_losses, val_losses = load_checkpoint_from_path(checkpoints_dir+f'/epoch{load_checkpoint}.pt', input_size, output_size, model_params=model_params)\n","        \n","    elif load_checkpoint and not checkpoints_dir:\n","        raise ValueError(\"If load_checkpoint is provided, checkpoints_dir must also be provided\")\n","    else:\n","        model = model_type(input_size, output_size, params=model_params)\n","        \n","        # Loss and optimizer\n","        lr = 1e-3\n","        optimizer = optim.Adam(model.parameters(), lr=lr)\n","    \n","        train_losses = []\n","        val_losses = []\n","    \n","    loss_fn = nn.MSELoss()\n","        \n","    for epoch in range(start_epoch, num_epochs+1):\n","        model.train()\n","        train_loss = 0\n","        for X_batch, y_batch in tqdm(train_loader):\n","            optimizer.zero_grad()\n","            outputs = model(X_batch)\n","            loss = loss_fn(outputs, y_batch)\n","            # loss = (loss * weights).mean()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            train_loss += loss.item()\n","        train_loss /= len(train_loader)\n","        \n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = 0.0\n","            for X_val_batch, y_val_batch in val_loader:\n","                val_outputs = model(X_val_batch)\n","                loss = loss_fn(val_outputs, y_val_batch)\n","                val_loss += loss.item()\n","            val_loss /= len(val_loader)\n","        \n","        train_losses.append(loss.item())\n","        val_losses.append(val_loss)\n","        if logging:\n","            print(f'Epoch [{epoch}/{num_epochs}]')\n","            print(f'\\tTraining Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","        elif save_epochs and epoch % save_epochs == 0 and not logging:\n","            print(f'Epoch [{epoch}/{num_epochs}]')\n","            print(f'\\tTraining Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","        \n","        if save_epochs and epoch % save_epochs == 0:\n","            weighting_type = 'equal'\n","            save_checkpoint(model, optimizer, epoch, train_losses, val_losses, lr, batch_size, weighting_type, f'{checkpoints_dir}/epoch{epoch}.pt')\n","        \n","        print()\n","    \n","    if final_save_path:\n","        final_save = {\n","            \"model_state_dict\": model.state_dict(),\n","            \"optimizer_state_dict\": optimizer.state_dict(),\n","            \"train_losses\": train_losses,\n","            \"val_losses\": val_losses,\n","            \"lr\": lr,\n","            \"batch_size\": batch_size,\n","            \"weighting_type\": 'equal',\n","            \"model_params\": model_params,\n","            \"model_type\": model_type,\n","            \"input_size\": input_size,\n","            \"output_size\": output_size,\n","            \"num_epochs\": num_epochs\n","        }\n","        torch.save(final_save, final_save_path)\n","    \n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def train_model_xgb(\n","        X_train, \n","        y_train, \n","        X_val, \n","        y_val, \n","        final_save_path=None, \n","        model_type=XGBRegressor):\n","    \n","    model = model_type(objective='reg:squarederror', device='cuda')\n","    model.fit(X_train, y_train)\n","    \n","    print(f\"Train MSE: {mean_squared_error(y_train, model.predict(X_train))}\")\n","    print(f\"Validation MSE: {mean_squared_error(y_val, model.predict(X_val))}\")\n","    \n","    if final_save_path:\n","        final_save = {\n","            \"model\": model\n","        }\n","        torch.save(final_save, final_save_path)\n","    \n","    return model"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def preprocess_data_LSTM(data, io_fn, seq_len, test_size_split=0.1):\n","    X, y = io_fn(data, seq_len)\n","    \n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size_split, shuffle=False)\n","    \n","    X_train = X_train.drop(columns=[f'date_lag{i}' for i in range(1, seq_len+1)])\n","    y_train = y_train.drop(columns=['date'])\n","    X_val = X_val.drop(columns=[f'date_lag{i}' for i in range(1, seq_len+1)])\n","    y_val = y_val.drop(columns=['date'])\n","    \n","    X_train = X_train.apply(pd.to_numeric, errors='coerce')\n","    y_train = y_train.apply(pd.to_numeric, errors='coerce')\n","    X_val = X_val.apply(pd.to_numeric, errors='coerce')\n","    y_val = y_val.apply(pd.to_numeric, errors='coerce')\n","    \n","    # Convert X_train of shape (n_samples, n_features) to (n_samples, seq_len, n_features)\n","    X_train = X_train.values.reshape(X_train.shape[0], seq_len, X_train.shape[1] // seq_len)[:, ::-1, :]\n","    y_train = y_train.values\n","    X_val = X_val.values.reshape(X_val.shape[0], seq_len, X_val.shape[1] // seq_len)[:, ::-1, :]\n","    y_val = y_val.values\n","    \n","    X_train = np.ascontiguousarray(X_train)\n","    X_val = np.ascontiguousarray(X_val)\n","    \n","    val_start_date = int(data.iloc[len(X_train)+seq_len]['date'])\n","    \n","    return X_train, y_train, X_val, y_val, val_start_date\n","\n","def preprocess_data_xgb(data, io_fn, seq_len, test_size_split=0.1):\n","    X, y = io_fn(data, seq_len)\n","    \n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size_split, shuffle=False)\n","    \n","    X_train = X_train.drop(columns=[f'date_lag{i}' for i in range(1, seq_len+1)])\n","    y_train = y_train.drop(columns=['date'])\n","    X_val = X_val.drop(columns=[f'date_lag{i}' for i in range(1, seq_len+1)])\n","    y_val = y_val.drop(columns=['date'])\n","    \n","    X_train = X_train.apply(pd.to_numeric, errors='coerce')\n","    y_train = y_train.apply(pd.to_numeric, errors='coerce')\n","    X_val = X_val.apply(pd.to_numeric, errors='coerce')\n","    y_val = y_val.apply(pd.to_numeric, errors='coerce')\n","    \n","    X_train = X_train.values\n","    y_train = y_train.values\n","    X_val = X_val.values\n","    y_val = y_val.values\n","    \n","    val_start_date = int(data.iloc[len(X_train)+seq_len]['date'])\n","    \n","    print(\"X_train.shape, y_train.shape, X_val.shape, y_val.shape\")\n","    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n","    print(f\"seq_len: {seq_len}\")\n","    \n","    \n","    return X_train, y_train, X_val, y_val, val_start_date"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["DATA_WEIGHING = \"value\"\n","NUM_INDUSTRIES = \"ff6\"\n","SEQ_LEN = 22\n","\n","def get_data(num_industries=49, weight_type='value', seed = 519, io_fn = io_day_n_lag, seq_len = SEQ_LEN):\n","    all_data_paths = {\n","        \"ff3\": \"ff3_daily.csv\",\n","        \"ff6\": \"ff6_daily.csv\"\n","    }\n","\n","    for n_ind in [5, 10, 12, 17, 30, 38, 48, 49]:\n","        for weight in ['equal', 'value']:\n","            all_data_paths[f\"{n_ind}industries_{weight}\"] = f\"ff_industry_portfolios_daily_{weight}/{n_ind}_Industry_Portfolios_Daily.csv\"\n","\n","    if num_industries in ['ff3', 'ff6']:\n","        data_path = all_data_paths[num_industries]\n","    else:\n","        data_path = all_data_paths[f'{num_industries}industries_{weight_type}']\n","\n","    data = pd.read_csv(data_path)\n","    data = data.dropna()\n","\n","    if seed is not None:\n","        set_seed(seed)\n","\n","    return data, io_fn, seq_len, all_data_paths"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["models_dirs = [\"models/max\", \"models/weighted\"]\n","_, io_fn, seq_len, all_data_paths = get_data(num_industries=NUM_INDUSTRIES, weight_type=DATA_WEIGHING)\n","\n","def predict_LSTM(model, X_val):\n","    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n","    y_val_tensor = torch.tensor(np.zeros((X_val.shape[0], X_val.shape[-1])), dtype=torch.float32)\n","\n","    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n","    val_loader = DataLoader(val_dataset, batch_size=32)\n","\n","    with torch.no_grad():\n","        pred_df = torch.tensor([])\n","        for X_val_batch, _ in val_loader:\n","            val_outputs = model(X_val_batch)\n","            \n","            pred_df = torch.cat((pred_df, val_outputs), 0)\n","            \n","        pred_df = pd.DataFrame(pred_df.numpy())\n","\n","    return pred_df\n","\n","def predict_xgboost(model, X_val):\n","    return pd.DataFrame(model.predict(X_val))\n","\n","def evaluate_model_LSTM(save_path, data_path, weighted=True, print_summary=True):\n","    data = pd.read_csv(data_path)\n","    data = data.dropna()\n","    \n","    _, _, X_val, y_val, val_start_date = preprocess_data_LSTM(data, io_fn, seq_len)\n","    \n","    model = load_model_LSTM(save_path)\n","    pred_df = predict_LSTM(model, X_val)\n","    print(pred_df.shape)\n","    \n","    strat_df, strat_ret_vec = predictions_to_returns(pred_df, y_val, weighted=weighted)\n","\n","    ff3 = load_ff3(val_start_date)\n","    strat_ret_vec.index = ff3.index\n","\n","    # Calculate alpha\n","    y_ols = sm.add_constant(ff3[['Mkt-RF','SMB', 'HML']])\n","    model_OLS = sm.OLS(strat_ret_vec, y_ols).fit()\n","    if print_summary:\n","        print(f\"Alpha: {model_OLS.params['const']:.3f} ({(np.power(1 + model_OLS.params['const']/100, 22*12) - 1) * 100:.2f}% annually)\")\n","        print(f\"Alpha p-value: {model_OLS.pvalues['const']:.3f}\")\n","        print(f\"Beta: {model_OLS.params['Mkt-RF']:.3f}\")\n","        print(f\"Beta p-value: {model_OLS.pvalues['Mkt-RF']:.3f}\")\n","    \n","    mkt_ret_vec = ff3['Mkt-RF']\n","    \n","    return strat_ret_vec, mkt_ret_vec\n","\n","def evaluate_model_xgb(save_path, data_path, weighted=True, print_summary=True):\n","    data = pd.read_csv(data_path)\n","    data = data.dropna()\n","    \n","    _, _, X_val, y_val, val_start_date = preprocess_data_xgb(data, io_fn, seq_len)\n","    \n","    model = load_model_xgb(save_path)\n","    pred_df = predict_xgboost(model, X_val)\n","    \n","    strat_df, strat_ret_vec = predictions_to_returns(pred_df, y_val, weighted=weighted)\n","\n","    ff3 = load_ff3(val_start_date)\n","    strat_ret_vec.index = ff3.index\n","\n","    # Calculate alpha\n","    y_ols = sm.add_constant(ff3[['Mkt-RF','SMB', 'HML']])\n","    model_OLS = sm.OLS(strat_ret_vec, y_ols).fit()\n","    if print_summary:\n","        print(f\"Alpha: {model_OLS.params['const']:.3f} ({(np.power(1 + model_OLS.params['const']/100, 22*12) - 1) * 100:.2f}% annually)\")\n","        print(f\"Alpha p-value: {model_OLS.pvalues['const']:.3f}\")\n","        print(f\"Beta: {model_OLS.params['Mkt-RF']:.3f}\")\n","        print(f\"Beta p-value: {model_OLS.pvalues['Mkt-RF']:.3f}\")\n","    \n","    strat_ret_vec = strat_ret_vec + ff3['RF']\n","    mkt_ret_vec = ff3['Mkt-RF'] + ff3['RF']\n","    \n","    return strat_ret_vec, mkt_ret_vec\n","\n","def show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name='Strategy', log_scale=False, get_cum_strat_rets=False):\n","    cum_strat_rets = (1 + strat_ret_vec/100).cumprod()\n","    cum_mkt_rets = (1 + mkt_ret_vec/100).cumprod()\n","    \n","    cum_mkt_rets.index = pd.to_datetime(cum_mkt_rets.index, format='%Y%m%d')\n","    cum_mkt_rets_monthly = cum_mkt_rets.resample('M').last()\n","    \n","    cum_strat_rets.index = pd.to_datetime(cum_strat_rets.index, format='%Y%m%d')\n","    cum_strat_rets_monthly = cum_strat_rets.resample('M').last()\n","    \n","    # Plot the cumulative returns\n","    plt.plot(cum_strat_rets_monthly, label=strategy_name)\n","    plt.plot(cum_mkt_rets_monthly, label='Market')\n","\n","    if log_scale:\n","        plt.yscale('log')\n","    plt.xlabel('Year')\n","    plt.ylabel('Cumulative Return')\n","    plt.legend()\n","    plt.show()\n","    \n","    if get_cum_strat_rets:\n","        return cum_strat_rets"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def chosen_strats_LSTM(save_path, data_path):\n","    data = pd.read_csv(data_path)\n","    data = data.dropna()\n","        \n","    _, _, X_val, y_val, _ = preprocess_data_LSTM(data, io_fn, seq_len)\n","        \n","    model = load_model_LSTM(save_path)\n","    pred_df = predict_LSTM(model, X_val)\n","    \n","    strat_df, _ = predictions_to_returns(pred_df, y_val)\n","    strat_df.columns = data.columns[1:]\n","\n","    return strat_df\n","\n","def chosen_strats_xgb(save_path, data_path):\n","    data = pd.read_csv(data_path)\n","    data = data.dropna()\n","        \n","    _, _, X_val, y_val, _ = preprocess_data_xgb(data, io_fn, seq_len)\n","        \n","    model = load_model_xgb(save_path)\n","    pred_df = predict_xgboost(model, X_val)\n","    \n","    strat_df, _ = predictions_to_returns(pred_df, y_val)\n","    strat_df.columns = data.columns[1:]\n","\n","    return strat_df"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def model_strat_distribution(model_path: str, data_path, data_weighing = None):\n","    idx = model_path.find(\"industries\")\n","    if idx != -1:\n","        strat = model_path[idx-2:idx] # 5, 10, 12, 17, 30, 38, 48, 49\n","        data_weighing = model_path[idx+11:idx+16] # equal or value\n","        data_path = all_data_paths[f'{strat}industries_{data_weighing}']\n","    else:\n","        strat = model_path[-6: -3] # ff3 or ff6\n","        data_path = all_data_paths[strat]\n","    \n","    data, _, _, _ = get_data(strat, data_weighing)\n","    \n","    if \"LSTM\" in model_path:\n","        model = load_model_LSTM(model_path)\n","        strat_df = chosen_strats_LSTM(model_path, data_path)\n","    elif \"XGB\" in model_path:\n","        model = load_model_xgb(model_path)\n","        strat_df = chosen_strats_xgb(model_path, data_path)\n","    else:\n","        raise ValueError(\"Model path must contain 'LSTM' or 'XGB'\")\n","        \n","    print(\"=====================================================================================================\")\n","    print(f\"Model: {model_path}\")\n","    print(\"=====================================================================================================\")\n","\n","    # create a dictionary from column names of strat_df\n","    strat_dist = {col: 0 for col in strat_df.columns}\n","    num_rows = len(strat_df)\n","    \n","    \n","    for row in strat_df.iterrows():\n","        # get the column name of the max value in the row\n","        max_strat_name = row[1][row[1] == True].index[0]\n","        strat_dist[max_strat_name] += 1\n","    \n","    strat_dist = dict(sorted(strat_dist.items(), key=lambda item: item[1], reverse=True))\n","    \n","    print(\"Most chosen strategies:\")\n","    for key in strat_dist.keys():\n","        print(f\"{key}: {strat_dist[key]/num_rows*100:.0f}%\", end=\", \")\n","    print()\n","    print()\n","    \n","    return strat_dist"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def preprocess_data(data, io_fn, seq_len, model_type):\n","    if model_type == SimpleLSTM or model_type == MultiLayerLSTM:\n","        return preprocess_data_LSTM(data, io_fn, seq_len)\n","    elif model_type == XGBRegressor or model_type == XGBRFRegressor:\n","        return preprocess_data_xgb(data, io_fn, seq_len)\n","    else:\n","        raise ValueError(\"Invalid model type\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def train_model(\n","    model_type,\n","    X_train, \n","    y_train, \n","    X_val, \n","    y_val, \n","    num_epochs=50, \n","    batch_size=32, \n","    logging=True, \n","    save_epochs=None, \n","    final_save_path=None,\n","    checkpoints_dir=None, \n","    load_checkpoint=None,\n","    model_params=None):\n","    if model_type == SimpleLSTM or model_type == MultiLayerLSTM:\n","        return train_model_LSTM(\n","            X_train, \n","            y_train, \n","            X_val, \n","            y_val, \n","            num_epochs=num_epochs, \n","            batch_size=batch_size, \n","            logging=logging, \n","            save_epochs=save_epochs, \n","            final_save_path=final_save_path,\n","            checkpoints_dir=checkpoints_dir, \n","            load_checkpoint=load_checkpoint,\n","            model_params=model_params,\n","            model_type=model_type)\n","    elif model_type == XGBRegressor or model_type == XGBRFRegressor:\n","        return train_model_xgb(\n","            X_train, \n","            y_train, \n","            X_val, \n","            y_val, \n","            final_save_path=final_save_path, \n","            model_type=model_type)\n","    else:\n","        raise ValueError(\"Invalid model type\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def train_all_models(\n","    model_types,\n","    ret_weighing_types,\n","    industries_list,\n","    io_fn,\n","    seq_len,\n","    num_epochs=50,\n","    batch_size=32,\n","    logging=False,\n","    save_epochs=None,\n","    final_save_path=None,\n","    checkpoints_dir=None,\n","    model_params=None):\n","    \n","    for model_type in model_types:\n","        for num_industries in industries_list:\n","            for data_weighing in [\"equal\", \"value\"]:\n","                for ret_weighing_type in ret_weighing_types:\n","                    # there is no 'equal' vs 'value' for ff3 and ff6\n","                    if num_industries in ['ff3', 'ff6'] and data_weighing == 'value':\n","                        pass\n","                    data, _, _, _ = get_data(num_industries, data_weighing)\n","                    X_train, y_train, X_val, y_val, val_start_date = preprocess_data(data, io_fn, seq_len, model_type)\n","                    print(\"X_train shape:\\t\", X_train.shape)\n","                    print(\"y_train shape:\\t\", y_train.shape)\n","                    print(\"X_val shape:\\t\", X_val.shape)\n","                    print(\"y_val shape:\\t\", y_val.shape)\n","                    print(\"val_start_date:\\t\", val_start_date)\n","                    \n","                    print(f\"Training {model_type} on {num_industries} industries with {data_weighing} weighting\")\n","                    \n","                    model_file_name = f\"{model_type.__name__}_{seq_len}days_{num_industries}\"\n","                    if num_industries not in ['ff3', 'ff6']:\n","                        model_file_name += f\"industries_{data_weighing}\"\n","                    \n","                    model = train_model(\n","                                model_type,\n","                                X_train, \n","                                y_train,\n","                                X_val,\n","                                y_val,\n","                                num_epochs,\n","                                batch_size,\n","                                logging=logging, \n","                                save_epochs=save_epochs,\n","                                final_save_path=f\"{final_save_path}/{ret_weighing_type}/{model_file_name}.pt\",\n","                                checkpoints_dir=f\"{checkpoints_dir}/{ret_weighing_type}/{model_file_name}\",\n","                                model_params=model_params)\n","                    \n","                    print(f\"Training complete. Saved {model_file_name} to {final_save_path}/{ret_weighing_type}\")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# seq_len = 22\n","# io_fn = io_day_n_lag\n","# model_type = SimpleLSTM\n","# num_industries = 'ff6'\n","# data_weighing = 'value'\n","# num_epochs = 50\n","# batch_size = 32\n","# logging = False\n","# save_epochs = 10\n","# final_save_path = \"models\"\n","# checkpoints_dir = \"checkpoints\"\n","# model_params = {\n","#     \"hidden_size\": 50,\n","#     \"num_layers\": 1,\n","#     \"dropout\": 0.2,\n","#     \"activation_fn\": nn.ReLU()\n","# }\n","# ret_weighing_type = \"weighted\"\n","\n","# data, _, _, _ = get_data('ff6', 'value')\n","# X_train, y_train, X_val, y_val, val_start_date = preprocess_data(data, io_fn, seq_len, model_type)\n","# print(\"X_train shape:\\t\", X_train.shape)\n","# print(\"y_train shape:\\t\", y_train.shape)\n","# print(\"X_val shape:\\t\", X_val.shape)\n","# print(\"y_val shape:\\t\", y_val.shape)\n","# print(\"val_start_date:\\t\", val_start_date)\n","\n","# print(f\"Training {model_type} on {num_industries} industries with {data_weighing} weighting\")\n","\n","# model_file_name = f\"{model_type.__name__}_{seq_len}days_{num_industries}\"\n","# if num_industries not in ['ff3', 'ff6']:\n","#     model_file_name += f\"industries_{data_weighing}\"\n","\n","# model = train_model(\n","#             model_type,\n","#             X_train, \n","#             y_train,\n","#             X_val,\n","#             y_val,\n","#             num_epochs,\n","#             batch_size,\n","#             logging=logging, \n","#             save_epochs=save_epochs,\n","#             final_save_path=f\"{final_save_path}/{ret_weighing_type}/{model_file_name}.pt\",\n","#             checkpoints_dir=f\"{checkpoints_dir}/{ret_weighing_type}/{model_file_name}\",\n","#             model_params=model_params)\n","\n","# print(f\"Training complete. Saved {model_file_name} to {final_save_path}/{ret_weighing_type}\")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["SEQ_LEN = 5\n","data, io_fn, seq_len, all_data_paths = get_data(NUM_INDUSTRIES, weight_type=DATA_WEIGHING, seq_len = SEQ_LEN)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# model_strat_distribution(\"models/weighted/SimpleLSTM_5days_ff6.pt\", all_data_paths['ff6'])\n","strat_ret_vec, mkt_ret_vec = evaluate_model_LSTM(f'models/weighted/SimpleLSTM_{SEQ_LEN}days_ff6.pt', all_data_paths['ff6'], print_summary='short', weighted=True)\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name=f'SimpleLSTM_{SEQ_LEN}days_ff6', log_scale=False)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# model_strat_distribution(\"models/weighted/SimpleLSTM_5days_ff6.pt\", all_data_paths['ff6'])\n","strat_ret_vec, mkt_ret_vec = evaluate_model_LSTM('models/max/SimpleLSTM_5days_ff6.pt', all_data_paths['ff6'], print_summary='short', weighted=False)\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name='SimpleLSTM_5days_ff6', log_scale=False)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# model_strat_distribution('models/weighted/XGBRegressor_5days_ff6.pt', all_data_paths['ff6'])\n","strat_ret_vec, mkt_ret_vec = evaluate_model_xgb('models/weighted/XGBRegressor_5days_ff6.pt', all_data_paths['ff6'], weighted=True, print_summary='short')\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name='XGBRegressor_5days_ff6', log_scale=False)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# model_strat_distribution('models/weighted/SimpleLSTM_5days_10industries_equal.pt', all_data_paths['10industries_equal'], data_weighing='equal')\n","strat_ret_vec, mkt_ret_vec = evaluate_model_LSTM('models/weighted/SimpleLSTM_5days_38industries_equal.pt', all_data_paths['38industries_equal'])\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name='SimpleLSTM_5days_38industries_equal', log_scale=False)\n","\n","# model_strat_distribution('models/weighted/SimpleLSTM_5days_10industries_value.pt', all_data_paths['10industries_value'], data_weighing='value')\n","strat_ret_vec, mkt_ret_vec = evaluate_model_LSTM('models/weighted/SimpleLSTM_5days_10industries_value.pt', all_data_paths['10industries_value'])\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name='SimpleLSTM_5days_10industries_value', log_scale=False)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# model_strat_distribution('models/weighted/XGBRegressor_5days_ff6.pt', all_data_paths['ff6'])\n","strat_ret_vec, mkt_ret_vec = evaluate_model_xgb('models/weighted/XGBRegressor_5days_10industries_equal.pt', all_data_paths['10industries_value'])\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name='XGBRegressor_5days_10industries_equal', log_scale=False)\n","\n","# model_strat_distribution('models/weighted/XGBRegressor_5days_10industries_value.pt',\n","# all_data_paths['10industries_value'])\n","strat_ret_vec, mkt_ret_vec = evaluate_model_xgb('models/weighted/XGBRegressor_5days_10industries_value.pt', all_data_paths['10industries_value'])\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name='XGBRegressor_5days_10industries_value', log_scale=False)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# model_strat_distribution('models/weighted/SimpleLSTM_5days_49industries_equal.pt', all_data_paths['49industries_equal'], data_weighing='equal')\n","strat_ret_vec, mkt_ret_vec = evaluate_model_LSTM('models/weighted/SimpleLSTM_5days_49industries_equal.pt', all_data_paths['49industries_equal'])\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name='SimpleLSTM_5days_49industries_equal', log_scale=False)\n","\n","# model_strat_distribution('models/weighted/SimpleLSTM_5days_49industries_value.pt', all_data_paths['49industries_value'], data_weighing='value')\n","strat_ret_vec, mkt_ret_vec = evaluate_model_LSTM(f'models/weighted/SimpleLSTM_5days_49industries_value.pt', all_data_paths[f'49industries_value'])\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name=f'SimpleLSTM_5days_49industries_value', log_scale=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model_strat_distribution('models/weighted/XGBRegressor_5days_49industries_equal.pt', all_data_paths['49industries_equal'], data_weighing='equal')\n","strat_ret_vec, mkt_ret_vec = evaluate_model_xgb(f'models/weighted/XGBRegressor_5days_49industries_equal.pt', all_data_paths[f'49industries_equal'])\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name='XGBRegressor_5days_49industries_equal', log_scale=False)\n","\n","# model_strat_distribution('models/weighted/XGBRegressor_5days_49industries_value.pt', all_data_paths['49industries_value'], data_weighing='value')\n","strat_ret_vec, mkt_ret_vec = evaluate_model_xgb(f'models/weighted/XGBRegressor_5days_49industries_value.pt', all_data_paths[f'49industries_value'])\n","show_cumulative_returns(strat_ret_vec, mkt_ret_vec, strategy_name='XGBRegressor_5days_49industries_value', log_scale=False)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":2}
